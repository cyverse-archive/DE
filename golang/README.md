# jobservices

The jobservices directory contains the code for multiple services whose collective purpose is to run and track asynchronous jobs triggered by user actions in the Discovery Environment, such as app launches and URL imports.

The code is written in the Go 1.5 programming language (https://golang.org/) and is built and tested with the **gb** tool (https://getgb.io/).

## Building

All of the job services and their required packages are built with a single invocation of the gb tool. Here's an example:

    gb build

That command places the executables in the **bin/** sub-directory. To build a version for another platform, set the **GOOS** environment variable (https://golang.org/doc/install/source#environment). Here's an example you would run on an OS X or Windows box if you're making a build for a Linux box:

    GOOS=linux gb build

CyVerse deploys the services as a Docker container image, even if some of the services don't run inside a container. To build the Docker image, run the docker-build.sh script.

    ./docker-build.sh

By default the script will attempt to push the image with a dev tag to the discoenv user's Docker repository. The **DOCKER_USER** environment variable can be used to override that setting.

    DOCKER_USER=johnworth ./docker-build.sh

## Running Tests

**gb** is used to run the unit tests.

    gb test

To run integration tests, use the **test.sh** script. It uses **docker-compose** and the **test.yml** file to spin up containers running RabbitMQ, PostgreSQL, and Docker-in-Docker.

    ./test.sh

If you notice connections failing, check if the **dedb** container started up properly. If **docker logs dedb** shows errors about SHMMAX, you may need to set it up with **--ipc host** (on some older systems, SHMMAX isn't properly inherited from the host system without this). Other sorts of failures may simply require a bit of a forced wait between starting the dedb, rabbit, and dind containers and running the test container.

## Workflow

A user submits a job through the UI. The job submission is assembled into JSON in the **apps** service, which posts the JSON to the **jex-adapter** service. The **jex-adapter** service sends the JSON as a message out on the **jobs.launches** topic of the **jobs** exchange.

**condor-launcher** receives the message, assembles the job submission for HTCondor, and submits the job by calling out to **condor_submit**. The submitted job is configured to run **road-runner**, passing it configuration files and the JSON job definition inside of a file. More information on these files is provided below.

HTCondor schedules the job for a slot in the cluster. It fires up **road-runner**, which reads in the job JSON and executes the job. It sends out multiple job status updates to the **jobs.updates** topic of the **jobs** exchange.

Each job status update is received by **job-status-recorder**, which saves the update to the **job_status_updates** table of the DE database.

Periodically, **job-status-to-apps-adapter** spins up and queries the database for any jobs that have updates that have not been propagated up to the UI yet. It posts those updates to the **apps** service, recording the status of each attempt.

At that point, **apps** forwards the update to the **notification-agent**, which makes it available to the UI and forwards completion notifications to the **iplant-email** service.

## Services

### condor-launcher

**condor-launcher** connects to the **jobs.launches** topic on the **jobs** exchange and launches jobs on the HTCondor cluster based on any messages that it receives.

When a message is received, condor-launcher creates a directory with this pattern: **/tmp/$username/$job-uuid/**. Inside this directory it creates multiple files: **irods-config**, **config**, **job**, **iplant.cmd**.

**irods-config** contains the settings that porklock needs to transfer files into and out of iRODS.

**config** contains the road-runner and RabbitMQ settings road-runner needs to run jobs and post job status updates.

**job** contains the job JSON generated by the **apps** service.

**iplant.cmd** contains the submission file that HTCondor requires for jobs to be submitted.

The above files and the **road-runner** executable are transferred to the HTCondor node that will run the job. The directory that is created is also the directory that contains the HTCondor logs generated by the jobs.

### jex-adapter

**jex-adapter** allows the **apps** service to send messages out on the **jobs.launches** topic without having to be modified. It does this by implementing the REST API from the old JEX service. **apps** is configured to hit this service instead of the old JEX.

### job-status-recorder

**job-status-recorder** listens for job status updates sent out by **road-runner** instances on the **jobs.updates** topic and records them in the **job_status_updates** table of the DE database.

### job-status-to-apps-adapter

**job-status-to-apps-adapter** periodically queries the **job_status_updates** table for any jobs that have updates that have not been posted to the **apps** service. It then iterates through all of those jobs, attempting to post the unposted updates and recording the result of the attempt back to the DE database.

### road-runner

**road-runner** executes jobs based on a JSON blob serialized to a file. Each step of the job runs inside a Docker container. Job results are transferred back into iRODS with the porklock tool. Job status updates are posted to the **jobs.updates** topic in the **jobs** exchange.
